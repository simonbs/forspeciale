\section{Approach}\label{sec:approach}
In our problem statement in \Cref{sec:researchstatement}, 
we asked what we can do with wearables in a smart home setting.
In the sections before that, 
we described some of the ways of controlling a smart home. 
One of these was using gestures to control devices in the home. 
As the most used sensor in wearables is the accelerometer (see \Cref{fig:wearables-sensors}), 
it makes sense to utilize this sensor to perform the gestures used to control the devices. 
Our approach to our problem statement is thus using a wearable, 
and its sensors, to perform gestures and thereby control a smart home. 

The goal of our project is to create a system, 
that allows gesture-based communication between the user and smart devices.
We want to design the system so that a user, wearing a \emph{wrist wearable}, 
can simply point at a smart device and perform a gesture to control this device. 
This is much like the solution that Reemo (see \Cref{sec:smarthomecontrol}) uses. 
However, we feel that the line-of-sight requirement is a big loss of control, 
as it severely limits the control of objects.
With a line-of-sight requirement, 
it is impossible to control objects in other rooms, 
such as turning on your coffee machine while you are in your bedroom. 
We want to remove this limitation in our solution. 

For this to work, we need a way to determine when a gesture is being performed, 
and which device should perform the action. 
Gesture recognition is a large subject and will be described in \Cref{sec:gesturerecognition}. 
Furthermore, to determine which device should perform the action, 
without a line of sight requirement, 
we need to know the \emph{indoor location} of the user, 
the position of the device(s) and the heading of the user's gesture. 
We have already discussed indoor location in \Cref{sec:indoor-positioning}, 
but will elaborate on how to use it for our project in \Cref{sec:designindoorlocation}.
At last, as described in \Cref{sec:smarthomes}, 
we need a smart hub to allow communication between the different devices, 
including the wearable, to be able to perform actions on the devices. 
This approach thus requires accurate indoor location, 
a wrist-worn wearable that can recognize gestures, 
and a hub for interoperability between the wearable and the smart devices. 

\subsection{Target Group}
Our proposed system can have various users. 
In this report so far, we have focused on smart homes, 
which correspond to a target group of people with smart homes. 
We argue that this group will be our main target group, 
as it is most likely to be used by people in that group for convenience of controlling their smart homes. 
People in this group will currently only be early adopters of technology, 
as the technology (smart homes/devices and wearables) has not yet become common in households.
Based on the trend of IoT, we think that this will no longer be the case in the near future (5-10 years). 
 
However, we also envision other groups to be our potential target groups.
One target group could be people whose work requires using an electronic, 
while not being near it or not wanting to touch it due to the situation,
\eg doctors operating and wanting to use e.g. a device to get further information.

It will also be able to help handicapped people. 
If someone is unable to walk, 
giving that person an easy way to control devices (that may even be out of reach) from afar, 
could provide a better life situation. 
In the same context, by using visual and/or vibrational feedback, 
the system could give blind people a way to determine what he or she is pointing at. 

The concept could even work in larger areas such as cities. 
Imagine being able to point at some building, or in some direction, 
and then receive a message what building that is or what is in that direction. 

The system would of course need to be modified or altered, 
so that these target groups would be able to use it, 
but we think that the concept is applicable in these situations. 
 